{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import needed libarires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./Dataset/train.csv\")\n",
    "train.drop(['discourse_id', 'essay_id'] , axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ########## Clean data\n",
    "def cleanText(df):\n",
    "\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    lengthArr = []\n",
    "    wordCountArr = []\n",
    "    sentCountArr = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "\n",
    "        curText = row.discourse_text\n",
    "\n",
    "        # lower case conversion\n",
    "        curText = curText.lower()\n",
    "        # removes all the trailing and leading spaces\n",
    "        curText = curText.strip()\n",
    "\n",
    "        # Saves the lengths of the writing from the model\n",
    "        lengthArr.append(len(curText))\n",
    "        wordCountArr.append(len(curText.split()))\n",
    "        #\n",
    "        sentCountArr.append(len([x for x in re.split(r\"[\\n\\.\\?\\!]+\", curText) if len(x) > 0]))\n",
    "\n",
    "        #Removes stop words\n",
    "        def remove_stop(x):\n",
    "            return \" \".join([word for word in str(x).split() if word not in stopWords])\n",
    "\n",
    "        curText = remove_stop(curText)\n",
    "\n",
    "        # removing all non alpha numeric char\n",
    "        curText = re.sub(r'[^a-z0-9 ]+', '', curText)\n",
    "        # removing \"...\" (multiple periods in a row)\n",
    "        curText = re.sub(r'([.])\\1+', '', curText)\n",
    "        # stems the text\n",
    "        curText = stemmer.stem(curText)\n",
    "\n",
    "        # removing multiple spaces in a row\n",
    "        curText = re.sub(r'(\\s\\s)+', ' ', curText)\n",
    "\n",
    "        # replaces the text\n",
    "        df.at[index, 'discourse_text'] = curText\n",
    "\n",
    "    df['StringLength'] = lengthArr\n",
    "    df['WordCount'] = wordCountArr\n",
    "    df['SentenceCount'] = sentCountArr\n",
    "\n",
    "\n",
    "cleanText(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Counts:  9636 \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abiol\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 2.64 GiB for an array with shape (36765, 9636) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [62]\u001b[0m, in \u001b[0;36m<cell line: 15>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature Counts: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(vector_features), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Prints the amount of words in the vectorizer\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Converts the vectorized data matrix to array\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m train_vec_arr \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_vectors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Puts the vectorized data into the dataframe\u001b[39;00m\n\u001b[0;32m     17\u001b[0m train_vec_dataframe \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data\u001b[38;5;241m=\u001b[39mtrain_vec_arr, columns\u001b[38;5;241m=\u001b[39mvector_features)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\sparse\\compressed.py:1039\u001b[0m, in \u001b[0;36m_cs_matrix.toarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m order \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1038\u001b[0m     order \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_swap(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcf\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 1039\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_toarray_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mc_contiguous \u001b[38;5;129;01mor\u001b[39;00m out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mf_contiguous):\n\u001b[0;32m   1041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutput array must be C or F contiguous\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py:1202\u001b[0m, in \u001b[0;36mspmatrix._process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m   1201\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 2.64 GiB for an array with shape (36765, 9636) and data type int64"
     ]
    }
   ],
   "source": [
    "# Vectorized strings\n",
    "countVec = CountVectorizer(\n",
    "    ngram_range=(1,2),\n",
    "    min_df=15\n",
    ")\n",
    "\n",
    "# Fits the vectorized with train data\n",
    "train_vectors = countVec.fit_transform(train['discourse_text'])\n",
    "\n",
    "# Gets a list of all the words in the vector\n",
    "vector_features = countVec.get_feature_names()\n",
    "# print(\"Vector features: \", vector_features)  # Prints all the words fit intoz the in the vectorizer\n",
    "print(\"Feature Counts: \", len(vector_features), \"\\n\\n\")  # Prints the amount of words in the vectorizer\n",
    "# Converts the vectorized data matrix to array\n",
    "train_vec_arr = train_vectors.toarray()\n",
    "# Puts the vectorized data into the dataframe\n",
    "train_vec_dataframe = pd.DataFrame(data=train_vec_arr, columns=vector_features)\n",
    "\n",
    "# One hot encodes discourse type\n",
    "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "X_categorical_OneHot_train = pd.DataFrame(OH_encoder.fit_transform(train[['discourse_type']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineDataFrame(dfOg, restDfs):\n",
    "    df = dfOg.copy()\n",
    "\n",
    "    # drops the text column as it has been vectorized and type since it's been one hot encoded\n",
    "    df.drop(['discourse_text', 'discourse_type'], inplace=True, axis=1)\n",
    "\n",
    "    if 'discourse_effectiveness'in df:\n",
    "        # ordinally encodes effectivness\n",
    "        df['discourse_effectiveness'] = dfOg[\"discourse_effectiveness\"].replace(\n",
    "            {\"Ineffective\": 0, \"Adequate\": 1, \"Effective\": 2}\n",
    "        )\n",
    "\n",
    "    for curDf in restDfs:\n",
    "        df = pd.concat([df, curDf], axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Gets the combined and fully cleaned model\n",
    "trainFullyCombined = combineDataFrame(train, [X_categorical_OneHot_train, train_vec_dataframe])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs oversampling\n",
    "ros = RandomOverSampler(sampling_strategy='auto', random_state=0)\n",
    "\n",
    "processedY = trainFullyCombined['discourse_effectiveness']\n",
    "trainFullyCombined.drop(['discourse_effectiveness'], axis=1, inplace=True)\n",
    "\n",
    "xResampled, yResampled = ros.fit_resample(trainFullyCombined, processedY)\n",
    "\n",
    "trainFullyProcessed = xResampled\n",
    "trainFullyProcessed['discourse_effectiveness'] = yResampled\n",
    "\n",
    "print(\"Categories After: \", train['discourse_effectiveness'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainFullyProcessed.head())\n",
    "trainFullyProcessed.to_csv('./Dataset/trainFullyProcessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if every letter is capitalized after every sentence\n",
    "def capitalize(text):\n",
    "    sentenceArr = [x for x in re.split(r\"[\\n\\.\\?\\!]+\", text) if len(x) > 0]\n",
    "    i = 0\n",
    "    upper_count = 0\n",
    "    for sentence in sentenceArr:\n",
    "        while i < len(sentence):\n",
    "            if sentence[i].isalpha():\n",
    "                if sentence[i].isupper():\n",
    "                    upper_count = upper_count + 1\n",
    "                else:\n",
    "                    break\n",
    "            i = i + 1\n",
    "    return (upper_count/len(sentenceArr)) * 100      \n",
    "            \n",
    "print(capitalize(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
